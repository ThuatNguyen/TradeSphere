# üìö T√†i li·ªáu API T√¨m ki·∫øm L·ª´a ƒë·∫£o

## üéØ T·ªïng quan

H·ªá th·ªëng t√¨m ki·∫øm l·ª´a ƒë·∫£o crawl d·ªØ li·ªáu t·ª´ 3 ngu·ªìn ch√≠nh:
1. **admin.vn** - Selenium scraping
2. **checkscam.vn** - Selenium scraping  
3. **chongluadao.vn** - API-based (kh√¥ng d√πng Selenium)

---

## üìÅ C·∫•u tr√∫c m√£ ngu·ªìn

```
TradeSphere/
‚îú‚îÄ‚îÄ server/
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pythonClient.ts          # Node.js client g·ªçi Python API
‚îÇ   ‚îî‚îÄ‚îÄ routes.ts                     # Express routes proxy ƒë·∫øn Python
‚îÇ
‚îú‚îÄ‚îÄ fastapi-service/                  # Python FastAPI Service
‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ       ‚îú‚îÄ‚îÄ main.py                   # FastAPI app ch√≠nh
‚îÇ       ‚îú‚îÄ‚îÄ services/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ crawler.py            # ‚≠ê M√É NGU·ªíN CRAWLER CH√çNH
‚îÇ       ‚îî‚îÄ‚îÄ api/v1/endpoints/
‚îÇ           ‚îî‚îÄ‚îÄ scams.py              # ‚≠ê API ENDPOINTS
‚îÇ
‚îî‚îÄ‚îÄ crawldata checkscam/              # Th∆∞ m·ª•c test crawler ƒë·ªôc l·∫≠p
    ‚îú‚îÄ‚îÄ main.py                       # Script test crawler
    ‚îî‚îÄ‚îÄ test_all_sources.py           # Test t·∫•t c·∫£ ngu·ªìn
```

---

## üîÑ Lu·ªìng ho·∫°t ƒë·ªông (Flow)

### 1Ô∏è‚É£ **User g·ªçi API t·ª´ frontend**
```
Frontend ‚Üí POST /api/scams/search?keyword=0123456789
```

### 2Ô∏è‚É£ **Express Server (Node.js) proxy request**
File: `server/routes.ts`
```typescript
app.get("/api/scams/search", async (req, res) => {
  const keyword = req.query.keyword as string;
  const type = req.query.type as string;
  
  // G·ªçi Python API
  const result = await searchScams(keyword, type);
  
  // L∆∞u v√†o database
  await storage.createScamSearch({...});
  
  res.json(result);
});
```

### 3Ô∏è‚É£ **Python Client g·ª≠i request ƒë·∫øn FastAPI**
File: `server/lib/pythonClient.ts`
```typescript
export async function searchScams(keyword: string, type?: string) {
  const response = await pythonAPI.get('/api/v1/scams/search', {
    params: { keyword, type },
  });
  return response.data;
}
```

### 4Ô∏è‚É£ **FastAPI x·ª≠ l√Ω request**
File: `fastapi-service/app/api/v1/endpoints/scams.py`
```python
@router.get("/search")
async def search_scams(keyword: str, type: Optional[str] = None):
    # 1. Check cache tr∆∞·ªõc
    cached_result = await cache_service.get_scam_search(keyword, type)
    if cached_result:
        return cached_result  # Tr·∫£ v·ªÅ lu√¥n n·∫øu c√≥ cache
    
    # 2. N·∫øu kh√¥ng c√≥ cache, crawl d·ªØ li·ªáu
    if type == "all":
        result = await crawler_service.search_all_sources(keyword)
    elif type == "admin":
        result = crawler_service.scrape_admin_vn(keyword, driver)
    # ...
    
    # 3. Cache k·∫øt qu·∫£
    await cache_service.set_scam_search(keyword, result, type)
    
    return result
```

### 5Ô∏è‚É£ **Crawler Service crawl d·ªØ li·ªáu**
File: `fastapi-service/app/services/crawler.py`

---

## üï∑Ô∏è Chi ti·∫øt Crawler

### A. **admin.vn Crawler**

**URL:** `https://admin.vn/scams?keyword={keyword}`

**Ph∆∞∆°ng ph√°p:** Selenium + BeautifulSoup

**Code:**
```python
def scrape_admin_vn(self, keyword: str, driver: webdriver.Chrome):
    # 1. Truy c·∫≠p trang web
    url = f"https://admin.vn/scams?keyword={keyword}"
    driver.get(url)
    
    # 2. ƒê·ª£i page load
    wait = WebDriverWait(driver, timeout)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "container")))
    
    # 3. Parse HTML v·ªõi BeautifulSoup
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # 4. T√¨m t·ªïng s·ªë k·∫øt qu·∫£
    alert_div = soup.find('div', class_='alert alert-danger text-center')
    total_scams = strong_tags[0].text.strip()  # "50 t·ªë c√°o"
    
    # 5. L·∫•y danh s√°ch scam cards
    scam_cards = soup.find_all('div', class_='scam-card')
    
    # 6. Parse t·ª´ng card
    for card in scam_cards:
        columns = card.find_all('div', class_='scam-column')
        scam_list.append({
            'name': columns[0].text.strip(),
            'amount': columns[1].text.strip(),
            'phone': columns[2].text.strip(),
            'account_number': columns[3].text.strip(),
            'bank': columns[4].text.strip(),
            'views': columns[5].text.strip(),
            'date': columns[6].text.strip(),
            'detail_link': card.find('a')['href']
        })
    
    return {
        'success': True,
        'source': 'admin.vn',
        'total_scams': total_scams,
        'data': scam_list
    }
```

**HTML Structure admin.vn:**
```html
<div class="alert alert-danger text-center">
    C√≥ <strong>50</strong> t·ªë c√°o li√™n quan ƒë·∫øn <strong>0123456789</strong>
</div>

<div class="scam-card">
    <div class="scam-column">Nguy·ªÖn VƒÉn A</div>    <!-- Name -->
    <div class="scam-column">5.000.000‚Ç´</div>       <!-- Amount -->
    <div class="scam-column">0123456789</div>       <!-- Phone -->
    <div class="scam-column">1234567890</div>       <!-- Account -->
    <div class="scam-column">Vietcombank</div>     <!-- Bank -->
    <div class="scam-column">150 l∆∞·ª£t xem</div>    <!-- Views -->
    <div class="scam-column">23/12/2025</div>      <!-- Date -->
    <a href="/scam/123" class="stretched-link"></a>
</div>
```

---

### B. **checkscam.vn Crawler**

**URL:** `https://checkscam.vn/?qh_ss={keyword}`

**Ph∆∞∆°ng ph√°p:** Selenium + BeautifulSoup

**Code:**
```python
def scrape_checkscam_vn(self, keyword: str, driver: webdriver.Chrome):
    # 1. Truy c·∫≠p
    url = f"https://checkscam.vn/?qh_ss={keyword}"
    driver.get(url)
    
    # 2. ƒê·ª£i load
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, "pst")))
    
    # 3. Parse HTML
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # 4. L·∫•y t·ªïng s·ªë
    h2_tag = soup.find('h2', class_='h1')
    # Text: 'C√≥ 25 c·∫£nh b√°o v·ªÅ "0123456789"'
    match = re.search(r'C√≥ (\d+) c·∫£nh b√°o', text)
    total_scams = match.group(1)
    
    # 5. L·∫•y danh s√°ch
    ct_divs = soup.find_all('div', class_='ct')
    
    for ct in ct_divs:
        ct1 = ct.find('div', class_='ct1')  # Title
        ct2 = ct.find('div', class_='ct2')  # Metadata
        
        link = ct1.find('a')
        name = link.text.strip()
        detail_link = link['href']
        
        spans = ct2.find_all('span')
        # Parse date v√† views t·ª´ spans
        
        scam_list.append({
            'name': name,
            'date': date,
            'views': views,
            'detail_link': detail_link
        })
    
    return {...}
```

**HTML Structure checkscam.vn:**
```html
<h2 class="h1">C√≥ 25 c·∫£nh b√°o v·ªÅ "0123456789"</h2>

<div class="ct">
    <div class="ct1">
        <a href="/canh-bao-123">L·ª´a ƒë·∫£o qua Facebook</a>
    </div>
    <div class="ct2">
        <span>L∆∞·ª£t xem 150</span>
        <span>3 th√°ng tr∆∞·ªõc</span>
    </div>
</div>
```

---

### C. **chongluadao.vn Crawler**

**URL:** `https://feeds.chongluadao.vn/checkmisc?q={keyword}`

**Ph∆∞∆°ng ph√°p:** API call (kh√¥ng d√πng Selenium)

**Code:**
```python
async def scrape_chongluadao_vn(self, keyword: str):
    # 1. Call API tr·ª±c ti·∫øp
    url = f"https://feeds.chongluadao.vn/checkmisc?q={keyword}"
    
    async with httpx.AsyncClient(timeout=10.0) as client:
        response = await client.get(url)
        data = response.json()
    
    # 2. Parse JSON response
    scam_list = []
    for item in data:
        source = item.get('source')      # 'scamvn' ho·∫∑c 'icallme'
        item_data = item.get('data')
        
        if source == 'scamvn':
            scam_list.append({
                'name': item_data.get('name'),
                'phone': item_data.get('phone'),
                'account': item_data.get('account'),
                'bank': item_data.get('bank'),
                'amount': item_data.get('amount'),
                'date': item_data.get('date'),
                'source': 'scamvn',
                'detail_link': item_data.get('link')
            })
        elif source == 'icallme':
            scam_list.append({
                'name': item_data.get('name'),
                'phone': item_data.get('phone'),
                'report_time': item_data.get('report_time'),
                'source': 'icallme',
                'detail_link': item_data.get('link')
            })
    
    return {...}
```

**API Response Format:**
```json
[
    {
        "source": "scamvn",
        "data": {
            "name": "Nguy·ªÖn VƒÉn A",
            "phone": "0123456789",
            "account": "1234567890",
            "bank": "Vietcombank",
            "amount": "5.000.000",
            "date": "2025-12-23",
            "link": "https://..."
        }
    },
    {
        "source": "icallme",
        "data": {
            "name": "Spam caller",
            "phone": "0123456789",
            "report_time": "2025-12-23",
            "link": "https://..."
        }
    }
]
```

---

### D. **Search All Sources (Parallel)**

**Code:**
```python
async def search_all_sources(self, keyword: str):
    # 1. Ch·∫°y song song 3 crawlers
    loop = asyncio.get_event_loop()
    
    # Selenium crawlers ch·∫°y trong thread pool
    future_admin = loop.run_in_executor(
        self.executor, 
        self.scrape_admin_vn, 
        keyword, 
        self.init_driver()
    )
    
    future_checkscam = loop.run_in_executor(
        self.executor, 
        self.scrape_checkscam_vn, 
        keyword, 
        self.init_driver()
    )
    
    # Async crawler
    future_chongluadao = self.scrape_chongluadao_vn(keyword)
    
    # 2. ƒê·ª£i t·∫•t c·∫£ ho√†n th√†nh
    results = await asyncio.gather(
        future_admin,
        future_checkscam,
        future_chongluadao,
        return_exceptions=True
    )
    
    # 3. T·ªïng h·ª£p k·∫øt qu·∫£
    total_results = 0
    for result in results:
        if result.get('success'):
            total_results += int(result.get('total_scams', 0))
    
    return {
        'success': True,
        'keyword': keyword,
        'total_results': total_results,
        'sources': results  # Array g·ªìm 3 sources
    }
```

---

## üöÄ API Endpoints

### 1. **Search All Sources**
```http
GET /api/v1/scams/search?keyword=0123456789&type=all
```

**Response:**
```json
{
    "success": true,
    "keyword": "0123456789",
    "total_results": 75,
    "cached": false,
    "response_time_ms": 3500,
    "sources": [
        {
            "success": true,
            "source": "admin.vn",
            "total_scams": "50",
            "data": [...]
        },
        {
            "success": true,
            "source": "checkscam.vn",
            "total_scams": "25",
            "data": [...]
        },
        {
            "success": true,
            "source": "chongluadao.vn",
            "total_scams": 0,
            "data": []
        }
    ]
}
```

### 2. **Search admin.vn Only**
```http
GET /api/v1/scams/search?keyword=0123456789&type=admin
```

### 3. **Search checkscam.vn Only**
```http
GET /api/v1/scams/search?keyword=0123456789&type=checkscam
```

### 4. **Search chongluadao.vn Only**
```http
GET /api/v1/scams/search?keyword=0123456789&type=chongluadao
```

---

## üíæ Caching Strategy

**Redis Cache:**
- Key format: `scam:search:{type}:{keyword}`
- TTL: 1 hour (configurable)
- Hit count tracking

**Code:**
```python
# Check cache tr∆∞·ªõc
cached_result = await cache_service.get_scam_search(keyword, source_type)
if cached_result:
    cached_result["cached"] = True
    return cached_result

# Crawl v√† cache
result = await crawler_service.search_all_sources(keyword)
await cache_service.set_scam_search(keyword, result, source_type)
```

---

## üîß Configuration

File: `fastapi-service/app/config.py`

```python
SELENIUM_HEADLESS = True          # Ch·∫°y Chrome headless
SELENIUM_TIMEOUT = 10             # Timeout cho Selenium
REDIS_URL = "redis://localhost"   # Redis cache
CACHE_TTL = 3600                  # Cache 1 gi·ªù
```

---

## üß™ Testing

### Test ri√™ng t·ª´ng ngu·ªìn:
```bash
cd "crawldata checkscam"
python test_all_sources.py
```

### Test qua API:
```bash
# Start FastAPI service
cd fastapi-service
uvicorn app.main:app --reload --port 8000

# Test
curl "http://localhost:8000/api/v1/scams/search?keyword=0123456789"
```

---

## üìä Database Logging

M·ªói search ƒë∆∞·ª£c log v√†o database:

**Table:** `scam_searches`
```sql
CREATE TABLE scam_searches (
    id SERIAL PRIMARY KEY,
    keyword TEXT NOT NULL,
    source TEXT,                 -- 'web' ho·∫∑c 'zalo'
    user_id INTEGER,
    zalo_user_id TEXT,
    results_count INTEGER,
    search_time TIMESTAMP DEFAULT NOW(),
    response_time_ms INTEGER
);
```

---

## üéØ C√°c ƒëi·ªÉm quan tr·ªçng

### ‚úÖ **∆Øu ƒëi·ªÉm:**
1. **Parallel crawling** - Crawl 3 ngu·ªìn ƒë·ªìng th·ªùi ‚Üí Nhanh h∆°n
2. **Redis caching** - Gi·∫£m t·∫£i, tƒÉng t·ªëc
3. **Thread pool** - Selenium kh√¥ng block async operations
4. **Error handling** - M·ªói source fail ri√™ng kh√¥ng ·∫£nh h∆∞·ªüng t·ªïng th·ªÉ
5. **Database logging** - Track usage v√† performance

### ‚ö†Ô∏è **L∆∞u √Ω:**
1. **Selenium dependencies** - C·∫ßn Chrome/Chromium v√† chromedriver
2. **Headless mode** - ƒê·ªÉ tr√°nh m·ªü browser khi production
3. **Timeout handling** - C·∫ßn set timeout h·ª£p l√Ω
4. **Rate limiting** - C·∫©n th·∫≠n khi crawl nhi·ªÅu request
5. **HTML structure changes** - Web thay ƒë·ªïi c√≥ th·ªÉ break crawler

---

## üêõ Troubleshooting

### Selenium kh√¥ng ch·∫°y:
```bash
# Install Chrome
sudo apt install chromium-browser

# Install chromedriver
pip install webdriver-manager
```

### Redis connection error:
```bash
# Start Redis
redis-server

# Check connection
redis-cli ping
```

### Crawler timeout:
- TƒÉng `SELENIUM_TIMEOUT` trong config
- Ki·ªÉm tra network connection
- Test manual v·ªõi browser

---

## üìö Tham kh·∫£o

- **Selenium Docs:** https://selenium-python.readthedocs.io/
- **BeautifulSoup Docs:** https://www.crummy.com/software/BeautifulSoup/
- **FastAPI Docs:** https://fastapi.tiangolo.com/
- **HTTPX Async Client:** https://www.python-httpx.org/

---

**Author:** TradeSphere Team  
**Last Updated:** December 23, 2025
